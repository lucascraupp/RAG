{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "O RAG é uma técnica que combina recuperação de informações (retrieval) com geração de linguagem natural (generation) para melhorar a qualidade e a precisão das respostas de modelos como o ChatGPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = \"gpt-4o-mini\"\n",
        "llm = OpenAI(model=model, temperature=0)\n",
        "tokenizer = tiktoken.encoding_for_model(model)\n",
        "\n",
        "collection_name = \"contracts_info\"\n",
        "\n",
        "question = \"O que fala a cláusula 7 do contrato de locação de maquinário?\"\n",
        "# question = \"O que fala a cláusula 7?\" # Usuário não especificou o contrato\n",
        "\n",
        "contents = []\n",
        "for file in os.listdir(\"docs\"):\n",
        "    if file.endswith(\".md\"):\n",
        "        with open(f\"docs/{file}\", \"r\") as f:\n",
        "            content = f.read()\n",
        "            contents.append({\"filename\": file, \"content\": content})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Acessando os dados sem RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens de entrada: 7756\n",
            "Tokens de saída: 128\n",
            "Total de tokens: 7884\n",
            "Resposta: assistant: A Cláusula 7 do contrato de locação de maquinário trata dos deveres da LOCATÁRIA. Ela se compromete a:\n",
            "\n",
            "a) Confiar à LOCADORA o direito de fiscalização do maquinário arrendado;\n",
            "b) Defender a posse e a propriedade das referidas máquinas;\n",
            "c) Manter sempre um mínimo de três funcionários treinados pela LOCADORA, para realização da execução dos serviços específicos do maquinário;\n",
            "d) Realizar o pagamento de quaisquer defeitos ou danos causados ao maquinário, bem como qualquer uma das máquinas pertencentes a este conjunto.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.prompts import ChatMessage\n",
        "\n",
        "non_rag_prompt = f\"\"\"\n",
        "    Você é um especialista em leitura e extração de dados em contratos. Seu papel é ler os documentos fornecidos e responder perguntas sobre eles.\n",
        "\n",
        "    Documentos:\n",
        "    {contents}\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=non_rag_prompt),\n",
        "    ChatMessage(role=\"user\", content=question),\n",
        "]\n",
        "\n",
        "response = llm.chat(messages)\n",
        "print(f\"Tokens de entrada: {len(tokenizer.encode(non_rag_prompt + question))}\")\n",
        "print(f\"Tokens de saída: {len(tokenizer.encode(str(response)))}\")\n",
        "print(\n",
        "    f\"Total de tokens: {len(tokenizer.encode(non_rag_prompt + question + str(response)))}\"\n",
        ")\n",
        "print(f\"Resposta: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Criação de uma base vetorial para o RAG usando ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens de entrada: 8476\n",
            "Tokens de saída: 9658\n",
            "Total de tokens: 18134\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "from llama_index.core.schema import TextNode\n",
        "from chromadb import PersistentClient\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "os.makedirs(\"database\", exist_ok=True)\n",
        "chroma_client = PersistentClient(path=\"database\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "input_tokens = 0\n",
        "output_tokens = 0\n",
        "\n",
        "for data in contents:\n",
        "    filename = data[\"filename\"]\n",
        "    content = data[\"content\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        Você é um especialista em leitura e extração de dados em contratos. Seu papel é ler o documento fornecido e separar blocos de texto com base em sua estrutura contratual.\n",
        "\n",
        "        Regras\n",
        "            - Não insira mais de uma cláusula dentro de um mesmo bloco;\n",
        "            - Nos metadados do bloco, sempre informe o filename ({filename});\n",
        "            - Se o conteúdo do bloco estiver inserido em uma seção ou cláusula identificável, inclua o campo \"section\" e/ou \"clausula\" nos metadados;\n",
        "            - Cada bloco de texto não deve ultrapassar 5.000 caracteres;\n",
        "            - A resposta deve ser uma lista JSON contendo os campos \"metadata\" e \"content\" para cada item;\n",
        "            - Não adicione comentários, interpretações ou dados externos;\n",
        "            - Não modifique ou omita nenhuma informação ou caractere do texto original.\n",
        "\n",
        "        Exemplo de saída:\n",
        "        [\n",
        "            {{\n",
        "                \"metadata\": {{\n",
        "                \"filename\": \"teste.docx\",\n",
        "                \"section\": \"IDENTIFICAÇÃO DAS PARTES CONTRATANTES\"\n",
        "                }},\n",
        "                \"content\": \"**CONTRATO DE TESTE**\\\\nIDENTIFICAÇÃO DAS PARTES CONTRATANTES\\\\nCONTRATANTE: (Nome da Contratante), ...\"\n",
        "            }},\n",
        "            {{\n",
        "                \"metadata\": {{\n",
        "                \"filename\": \"teste.docx\",\n",
        "                \"section\": \"DO OBJETO DO CONTRATO\",\n",
        "                \"clausula\": \"1ª\"\n",
        "                }},\n",
        "                \"content\": \"Cláusula 1ª. O presente contrato tem como OBJETO, ...\"\n",
        "            }},\n",
        "            {{\n",
        "                \"metadata\": {{\n",
        "                \"filename\": \"teste.docx\",\n",
        "                }},\n",
        "                \"content\": \"Por estarem assim justos e contratados, ...\"\n",
        "            }}\n",
        "        ]\n",
        "\n",
        "        Documento:\n",
        "        {content}\n",
        "    \"\"\"\n",
        "\n",
        "    i = 0\n",
        "    while i < 3:\n",
        "        try:\n",
        "            response = llm.complete(prompt).text\n",
        "            break\n",
        "        except Exception as e:\n",
        "            i += 1\n",
        "\n",
        "    if \"```json\" in response:\n",
        "        response = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "    elif \"```\" in response:\n",
        "        response = response.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "    chunks_list = json.loads(response)\n",
        "\n",
        "    input_tokens += len(tokenizer.encode(prompt))\n",
        "    output_tokens += len(tokenizer.encode(response))\n",
        "\n",
        "    nodes = []\n",
        "    for chunk in chunks_list:\n",
        "        metadata = chunk.get(\"metadata\", {})\n",
        "        content = chunk.get(\"content\", \"\")\n",
        "\n",
        "        node = TextNode(text=content, metadata=metadata)\n",
        "        nodes.append(node)\n",
        "\n",
        "    VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "print(f\"Tokens de entrada: {input_tokens}\")\n",
        "print(f\"Tokens de saída: {output_tokens}\")\n",
        "print(f\"Total de tokens: {input_tokens + output_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG com ChromaDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Acessando os dados com RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens de entrada: 242\n",
            "Tokens de saída: 84\n",
            "Total de tokens: 326\n",
            "Número de chunks recuperados: 2\n",
            "Resposta: A cláusula 7 do contrato de locação de maquinário estabelece os deveres da locatária. A locatária deve comunicar imediatamente qualquer ameaça ao maquinário, permitir a fiscalização pela locadora, defender a posse e propriedade das máquinas, manter pelo menos três funcionários treinados pela locadora para operar o maquinário, e pagar por quaisquer defeitos ou danos causados ao maquinário.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
        "from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import ResponseMode\n",
        "\n",
        "chroma_client = PersistentClient(path=\"database\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "\n",
        "retriever = VectorIndexRetriever(index=index)\n",
        "\n",
        "retrieved_nodes = retriever.retrieve(question)\n",
        "\n",
        "context_content = \"\"\n",
        "for node in retrieved_nodes:\n",
        "    context_content += node.text + \"\\n\"\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    retriever=retriever,\n",
        "    llm=llm,\n",
        "    response_mode=ResponseMode(\"compact\"),\n",
        ")\n",
        "\n",
        "response = query_engine.query(question)\n",
        "\n",
        "print(f\"Tokens de entrada: {len(tokenizer.encode(question + context_content))}\")\n",
        "print(f\"Tokens de saída: {len(tokenizer.encode(str(response)))}\")\n",
        "print(\n",
        "    f\"Total de tokens: {len(tokenizer.encode(question + context_content + str(response)))}\"\n",
        ")\n",
        "print(f\"Número de chunks recuperados: {len(retrieved_nodes)}\")\n",
        "print(f\"Resposta: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filtrando os chunks usando seus metadados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens de entrada: 18\n",
            "Tokens de saída: 2\n",
            "Total de tokens: 20\n",
            "Número de chunks recuperados: 0\n",
            "Resposta: Empty Response\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
        "from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import ResponseMode\n",
        "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters\n",
        "\n",
        "chroma_client = PersistentClient(path=\"database\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "\n",
        "filters = MetadataFilters(\n",
        "    filters=[\n",
        "        MetadataFilter(key=\"filename\", value=\"docs/Terraplanagem.md\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever = VectorIndexRetriever(index=index, filters=filters)\n",
        "\n",
        "retrieved_nodes = retriever.retrieve(question)\n",
        "\n",
        "context_content = \"\"\n",
        "for node in retrieved_nodes:\n",
        "    context_content += node.text + \"\\n\"\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    retriever=retriever,\n",
        "    llm=llm,\n",
        "    response_mode=ResponseMode(\"compact\"),\n",
        ")\n",
        "\n",
        "response = query_engine.query(question)\n",
        "\n",
        "print(f\"Tokens de entrada: {len(tokenizer.encode(question + context_content))}\")\n",
        "print(f\"Tokens de saída: {len(tokenizer.encode(str(response)))}\")\n",
        "print(\n",
        "    f\"Total de tokens: {len(tokenizer.encode(question + context_content + str(response)))}\"\n",
        ")\n",
        "print(f\"Número de chunks recuperados: {len(retrieved_nodes)}\")\n",
        "print(f\"Resposta: {response}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
